{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a81c756",
   "metadata": {},
   "source": [
    "# Why are delta-AUC-PR scores so different across datasets?\n",
    "The output of the notebook 4.3 shows that there is vastly different performance for our pipeline depending upon which dataset is being used. Prism results in delta-AUC-PR scores that are marginally better than chance. However, the delta-AUC-PR scores are significantly better for toxvaldb (0.2-0.3). What is the reason for this vastly different performance aross support set sizes?\n",
    "\n",
    "I will start by exploring some of the following questions:\n",
    "- Are there some types of assays (e.g. aquatic predictions) for which toxvaldb has significantly better classification performance?\n",
    "- Does the average assay across datasets have a similar active ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9d8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40a700",
   "metadata": {},
   "source": [
    "## Assay makeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c91327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_info(filepath):\n",
    "\n",
    "    # Split the string at each slash to isolate each part of the path\n",
    "    parts = filepath.split(\"/\")\n",
    "\n",
    "    # The target part is the one that contains 'params.dataset' and 'params.support_set_size'\n",
    "    target = [part for part in parts if \"params.dataset\" in part and \"params.support_set_size\" in part][0]\n",
    "\n",
    "    # Split the target part at each comma\n",
    "    params = target.split(\",\")\n",
    "\n",
    "    # Split each parameter at the equals sign and take the second part\n",
    "    dataset = params[0].split(\"=\")[1]\n",
    "    support_set_size = int(params[1].split(\"=\")[1])  # convert to int for numerical operations\n",
    "\n",
    "    return {\"dataset\": dataset, \"support_set_size\": support_set_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baee352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for clintox with support set size of 16\n",
      "No data for bbbp with support set size of 32\n",
      "No data for tox21 with support set size of 64\n",
      "No data for bbbp with support set size of 16\n",
      "No data for clintox with support set size of 32\n",
      "No data for bbbp with support set size of 64\n",
      "No data for tox21 with support set size of 16\n",
      "No data for tox21 with support set size of 32\n",
      "No data for clintox with support set size of 64\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"/Users/sethhowes/Desktop/FS-Tox/multirun/2023-07-19/11-23-46\"\n",
    "run_dirs = [os.path.join(INPUT_DIR, run_dir) for run_dir in os.listdir(INPUT_DIR) if os.path.isdir(os.path.join(INPUT_DIR, run_dir))]\n",
    "run_dirs = [f\"{run_dir}/data/processed/score/*.parquet\" for run_dir in run_dirs]\n",
    "\n",
    "support_sizes = [8, 16, 32, 64]\n",
    "datasets = [\"clintox\", \"tox21\", \"toxcast\", \"bbbp\", \"toxval\", \"nci60\", \"cancerrx\", \"prism\"]\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    info = get_run_info(run_dir)\n",
    "    query = f\"\"\"\n",
    "    SELECT delta_auc_pr\n",
    "    FROM read_parquet('{run_dir}')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = con.execute(query).df()\n",
    "        df[\"support_set_size\"] = info[\"support_set_size\"]\n",
    "        df[\"dataset\"] = info[\"dataset\"]\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"No data for {info['dataset']} with support set size of {info['support_set_size']}\")\n",
    "\n",
    "        \n",
    "# Concatenate all dataframes into one\n",
    "df_final = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c7d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs-tox",
   "language": "python",
   "name": "fs-tox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
